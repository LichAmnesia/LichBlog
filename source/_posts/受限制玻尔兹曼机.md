---
title: 受限制玻尔兹曼机
date: 2016-03-28 22:21:02
tags:
    - 机器学习
    - mathematic
---

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="http://music.163.com/outchain/player?type=2&id=406238&auto=0&height=66"></iframe>

本文主要介绍了RBM(受限制玻尔兹曼机)。首先介绍背景知识，然后介绍了RBM的结构。最后讲了一下它为什么是一种DL方法，引出DBN。

<!-- more -->

# 1. 背景介绍
### 1.1玻尔兹曼机介绍
玻尔兹曼机是一个对称连接,利用类似神经元作单位来随机决定开关的网络(随机的意思是随机确定的)。
玻尔兹曼机（Boltzmann machine）是随机神经网络和递归神经网络的一种。

玻尔兹曼机可被视作随机过程的，可生成的相应的Hopfield神经网络。它是最早能够学习内部表达，并能表达和（给定充足的时间）解决复杂的组合优化问题的神经网络。但是，没有特定限制连接方式的玻尔兹曼机目前为止并未被证明对机器学习的实际问题有什么用。所以它目前只在理论上显得有趣。然而，由于局部性和训练算法的赫布性质(Hebbian nature)，以及它们和简单物理过程相似的并行性，如果连接方式是受约束的（即约束玻尔兹曼机），学习方式在解决实际问题上将会足够高效。
### 1.2 赫布理论
赫布理论（英语：Hebbian theory）是一个神经科学理论，解释了在学习的过程中脑中的神经元所发生的变化。理论表述：

>我们可以假定，反射活动的持续与重复会导致神经元稳定性的持久性提升……当神经元A的轴突与神经元B很近并参与了对B的重复持续的兴奋时，这两个神经元或其中一个便会发生某些生长过程或代谢变化，致使A作为能使B兴奋的细胞之一，它的效能增强了。

### 1.3 Hopfiled神经网络
Hopfiled神经网络是一种递归神经网络，由约翰·霍普菲尔德在1982年发明。Hopfield网络是一种结合存储系统和二元系统的神经网络。它保证了向局部极小的收敛，但收敛到错误的局部极小值（local minimum），而非全局极小（global minimum）的情况也可能发生。

Hopfiled网络的单元是二次元的，即，这些单元只能接受两个不同的值，并且值取决于输入的大小是否达到阈值。Hopfield网络通常接受值为-1或1，也可以是0或者1。输入是由sigmoid方程处理得到的。输入是为了用于将输入化简为两个极值。
下图是一个四个顶点的Hopfiled神经网络示意图：
![hopfiled](http://7xrh75.com1.z0.glb.clouddn.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_Hopfield-net.png)

每一对Hopfiled网络的单元i和j间都有一对以一定权重（weight）的连接$ w\_{ij} $ 。因此，Hopfiled网络可被描述为一个完整的无向图$ G = < V, f \> $，其中V是人工神经元集合。
Hopfiled网络的连接有以下特征：
$w\_{ii}=0, \forall i $（没有神经元和自身相连）
$w\_{ij} = w\_{ji}, \forall{i,j} $（连接权重是对称的）

权重对称的要求是一个重要特征，因为它保证了能量方程（称向函数某一点收敛的过程为势能转化为能量）在神经元激活时单调递减，而不对称的权重可能导致周期性的递增或者噪声。然而，Hopfiled网络也证明噪声过程会被局限在很小的范围，并且并不影响网络的最终性能。

# 2. RBM介绍
受限玻尔兹曼机（restricted Boltzmann machine, RBM）是一种可通过输入数据集学习概率分布的随机生成神经网络。
可以应用于分类、协同过滤、特征学习和主题建模。根据任务的不同，受限玻尔兹曼机可以使用监督学习或无监督学习的方法进行训练。

正如名字所提示的那样，受限玻尔兹曼机是一种玻尔兹曼机的变体，但限定模型必须为二分图。模型中包含对应输入参数的输入（可见）单元和对应训练结果的隐单元，图中的每条边必须连接一个可见单元和一个隐单元。（与此相对，“无限制”玻尔兹曼机包含隐单元间的边，使之成为递归神经网络。）这一限定使得相比一般玻尔兹曼机更高效的训练算法成为可能，特别是基于梯度的对比分歧（contrastive divergence）算法。
受限玻尔兹曼机也可被用于**深度学习网络**。具体地，深度信念网络可使用**多个RBM**堆叠而成，并可使用**梯度下降法**和**反向传播算法**进行调优。

# 3. 结构
![RBM网络结构](http://7xrh75.com1.z0.glb.clouddn.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_Restricted_Boltzmann_machine.svg.png)
上图很好地说明受限玻尔兹曼机的结构是怎样的。输入层和隐含层相当于二部图。每一层之间是没有连接的。V表示的是可视层，即输入数据层。h表示的是隐藏层。并且假设所有节点都是随机二值变量节点（0或者1），同时满足全概率分布$p(v,h)$的玻尔兹曼分布。就称这个模型是RBM。

RBM是一个Deep Learning的方法。原因如下，首先是一个二部图，在已知V的情况下，所有隐藏层的节点之间是条件独立的，也就是：
$$ p(h|V) = p(h\_1|V) \times p(h\_2|V) \dots p(h\_n|V)$$
同理如果已知隐藏层h那么V的节点都是条件独立的。同时又由于所有的v和h都满足玻尔兹曼分布，因此当输入V的时候，可以通过$p(h|v)$得到h层，而得到隐藏层之后，通过$p(v|h)$又能得到可视层。通过调整参数，让隐藏层得到的可视层$V\_1$和之间的V是一样的，那么隐藏层就是可视层的另一种表达。因此隐藏层可以作为可视层输入数据的特征。所以就是一种DL的方法。

具体推导可以参考这篇[博客](http://blog.csdn.net/zouxy09/article/details/8781396)。

![DBN](http://7xrh75.com1.z0.glb.clouddn.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_1365561611_3496.jpg)

如果，我们把隐藏层的层数增加，我们可以得到Deep Boltzmann Machine(DBM)；如果我们在靠近可视层的部分使用贝叶斯信念网络（即有向图模型，当然这里依然限制层中节点之间没有链接），而在最远离可视层的部分使用Restricted Boltzmann Machine，我们可以得到DeepBelief Net（DBN）。
# 参考资料
[1] 赫布理论：https://zh.wikipedia.org/wiki/%E8%B5%AB%E5%B8%83%E7%90%86%E8%AE%BA

[2] Hopfield神经网络：https://zh.wikipedia.org/wiki/Hopfield%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C

[3] 受限玻尔兹曼机：https://zh.wikipedia.org/wiki/%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA

[4] 深度学习读书笔记之RBM（限制波尔兹曼机）：http://blog.csdn.net/mytestmy/article/details/9150213


----
　

因为我们是朋友，所以你可以使用我的文字，但请注明出处：http://alwa.info