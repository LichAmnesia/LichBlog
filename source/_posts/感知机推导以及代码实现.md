---
title: 感知机推导以及代码实现
date: 2016-04-12 18:29:13
tags:
    - 机器学习
    - mathematic
---

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="http://music.163.com/outchain/player?type=2&id=27032782&auto=0&height=66"></iframe>

　　本文介绍感知机定义以及推导过程，并使用python代码实现。也介绍了对偶形式的实现和原理。

<!-- more -->

# 1. 感知机定义
　　感知机(perceptron)是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应输入空间将实例划分为正负两类的分离超平面，属于判别模型。
　　感知机学习目的在于求出将训练数据进行线性划分的分离超平面，为此导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。

　　给定一个n维输入$x = (x\_1,x\_2,\dots,x\_n)$。
$$
\hat y = 
\begin{equation}
\begin{cases}
\+ 1 \ \ \ 当 W^T X + b > 0\\\\
\- 1 \ \ \ 当 W^T X + b \le 0
\end{cases}
\end{equation}
$$

　　其中，$W$是n维的权重向量，b是偏置项。W和b是未知的，需要从给定的训练数据学习得到。
　　如果使用增广的输入和权重向量，那么公式简写为：
$$
\hat y = 
\begin{equation}
\begin{cases}
\+ 1 \ \ \ 当 W^T X > 0 \\\\
\- 1 \ \ \ 当 W^T X \le 0
\end{cases}
\end{equation}
$$

# 2. 学习算法
　　如果给定N个训练样本$(X\_i,y\_i), i = 1,\dots,N$，我们希望学习得到$W^{\*}$。
$$
\begin{align}
W^{\*T} X\_i > 0  \ \ \ 当 y\_i > 0 \\\\
W^{\*T} X\_i < 0  \ \ \ 当 y\_i < 0
\end{align}
$$
　　公式等价于$W^{\*T} (y\_iX\_i) > 0$

　　假设训练数据集是线性可分的，感知机学习目的求一个能够将训练集正负实例点完全正确分开的分离超平面。为了找到这样的超平面，我们需要确定学习策略，即定义损失函数，并将损失函数极小化。
　　损失函数选择为误分类点到超平面S的总距离。首先输入空间$R^n$中任一点$x\_0$到超平面S距离：
$$
\frac{1}{||W||}|W^T X\_0 + b|
$$
　　这里$||W||$是$W$的$L\_2$范数。
　　对于误分类的数据$(X\_i,y\_i)$来说，
$$
\- y\_i (W^T X\_i + b) > 0
$$
　　用增广的输入和权重向量那就是，以下都用增广向量表示：
$$
\- y\_i (W^T X\_i) > 0
$$
　　因为当$W^T X\_i > 0, y\_i = -1$，当$W^T X\_i < 0, y\_i = +1$，所以可以得到误分类点到超平面S的总距离。假设超平面S的误分类点集合为M。
$$
\-\frac{1}{||W||}\sum\_{x\_i \in M} y\_i (W^T X\_i)
$$
　　不考虑$\frac{1}{||W||}$，就得到感知机($sign(W^T X)$)学习的损失函数。
$$
L(y,f(X,W)) = - \sum\_{x\_i \in M} y\_i (W^T X\_i)
$$
　　M是超平面误分类点集合。
　　显然损失函数$L(y,f(X,W))$是非负的，并且给定训练数据集，损失函数是连续可导函数。可以使用梯度下降法进行求解。

# 3. 随机梯度下降法求解
　　目前问题已经化解为求解如下函数的极值问题。
$$
\min\_{W} L(y,f(X,W)) = - \sum\_{x\_i \in M} y\_i (W^T X\_i)
$$
　　首先任意选取一个超平面$W\_0$(包括w,和b的增广向量)
$$
\nabla\_w L(y,f(X,W)) = - \sum\_{x\_i \in M} y\_i X\_i
$$
　　每次迭代，随机选取一个误分类点$(X\_i,y\_i)$，也就是
$$
W\_{k+1} \leftarrow W\_k + \lambda y\_i X\_i
$$
　　其中$\lambda$是步长，通过迭代可以期待损失函数不断减小，直到0为止。
# 4. 算法原始形式

　　输入：$T=\\{(X\_1,y\_1),(X\_2,y\_2)...(X\_N,y\_N)\\}$,其中$X\_i \in X = R^n，yi \in Y = {-1, +1}，i = 1,2...N$，学习速率为$\lambda$。
　　输出：$W^{\*}$;
　　感知机模型（增广向量形式）：
$$
f(x)=f(x)= sign(W^T X + b)  = sign(W^T X)
$$
　　(1) 初始化$W\_0$
　　(2) 在训练数据集中选取$(X\_i, y\_i)$
　　(3) 如果$y\_i (W^T X\_i) ≤ 0$
$$
W = W + \lambda y\_i X\_i
$$
　　(4) 转至（2）
　　
　　下面是一个感知机学习过程图：
![感知机](http://7xrh75.com1.z0.glb.clouddn.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_%E6%84%9F%E7%9F%A5%E6%9C%BA.jpg)

# 5. 对偶形式
　　对偶形式的基本想法是，将W和b表示为实例$X\_i$和标记$y\_i$的线性组合的形式，通过求解其系数而得到W和b。假设按照之前的修改过程经过n次。那么W，b关于$(x\_i,y\_i)$的增量为$a\_iy\_ix\_i$和$a\_iy\_i$，其中$a\_i=n\_i\lambda$。那么最后的W和b可以表示：
$$
\begin{align}
W = \sum\_{i = 1}^{N} a\_i y \_i x\_i \\\\
b = \sum\_{i = 1}^{N} a\_i y \_i
\end{align}
$$
　　如果是增广形式，那么就是。
$$
\begin{align}
W = \sum\_{i = 1}^{N} a\_i y \_i x\_i \\\\
\end{align}
$$
　　这里的$a\_i \ge 0, i = 1,2,\dots,N，当\lambda=1$,表示第i个实例点犹豫误分类而进行更新的次数，实例点更新次数越多，那么它距离分离超平面越近，也就越难分类。换句话，这些事例对学习结果影响最大。
　　输入：$T = \\{ (X\_1,y\_1),(X\_2,y\_2)...(X\_N,y\_N) \\} $ 其中$X\_i \in X = R^n，yi \in Y = {-1, +1}，i = 1,2...N$，学习速率为$\lambda$。
　　输出：a,b;
　　感知机模型
$$
f(x)=f(x)= sign(W^T X + b)  = sign(W^T X)
$$
　　(1) 初始化$w\_0,b\_0$
　　(2) 在训练数据集中选取$(x\_i, y\_i)$
　　(3) 如果 $y\_i (\sum\_{j=1}^{N}a\_jy\_jx\_j \cdot x\_i + b) \le 0$
$$
\begin{align}
a\_i = a\_i + \lambda \\\\
b = b + \lambda y\_i
\end{align}
$$
        
　　(4) 转至（2）

　　对偶形式中训练数据仅以内积的形式出现，为了方便可以预先把训练数据间内积计算出来并以矩阵的形式存储起来，这个矩阵就是所谓的Gram矩阵。
　　$$
　　G = [X\_i \cdot X\_j]\_{N \times N}
　　$$
# 6. 代码实现
　　整个代码放到我的[GitHub](https://github.com/LichAmnesia/MachineLearningTools)上了。分为train和test，每个都是500行数据。train做训练集。
>	trainFile = 'data/perceptron_train.txt'
	testFile = 'data/perceptron_test.txt'

　　数据格式为：
>0.94544 0.42842 0.79833 0.16244	-1
0.85365 0.084168 0.5682 0.49221	-1
0.17095 0.82127 0.98444 0.51486	-1
0.51412 0.92124 0.42323 0.097934	-1
0.28147 0.71434 0.075309 0.9116	1
0.46295 0.64512 0.96324 0.31516	-1
0.97789 0.80155 0.90235 0.74203	-1
0.41825 0.69419 0.46246 0.31523	-1
0.75203 0.20264 0.8765 0.47593	-1

　　程序源码：
```python
# -*- coding: utf-8 -*-
# @Author: Lich_Amnesia  
# @Email: alwaysxiaop@gmail.com
# @Date:   2016-04-12 15:40:14
# @Last Modified time: 2016-04-12 18:25:20
# @FileName: Perceptron.py


import numpy as np
import matplotlib.pyplot as plt
import sys,os

# process the origin data to formatted train/test data like the following
# train.txt
# 0.28147 0.71434 0.075309 0.9116	1
# 0.46295 0.64512 0.96324 0.31516	-1
# Also random generate train/test dataset. 
# All the dataset size is 500, 500 for test set and other for training.
# load data from train/test file.
def loadData():
	trainFile = 'data/perceptron_train.txt'
	testFile = 'data/perceptron_test.txt'
	trainDataSet = np.loadtxt(open(trainFile,"rb"))
	testDataSet = np.loadtxt(open(testFile,"rb"))
	# Add X0 to the dataSet
	X0 = np.array([1.0 for i in range(trainDataSet.shape[0])])
	trainDataSet = np.c_[X0,trainDataSet]
	X0 = np.array([1.0 for i in range(testDataSet.shape[0])])
	testDataSet = np.c_[X0,testDataSet]
	return trainDataSet, testDataSet

# normalization, 
def norm(input_x):
	mean = np.mean(input_x,axis=0)
	std = np.std(input_x,axis=0)
	n, m = input_x.shape
	for i in range(n):
		for j in range(m):
			if std[j] != 0:
				input_x[i][j] = (input_x[i][j] - mean[j]) / std[j]
	return input_x
	

class Perceptron:
	def __init__(self, W, alpha, eps = 1e-8):
		self.W = np.mat(W)
		self.alpha = alpha
		self.eps = eps

	def loss(self, x, y):
		return y * (np.dot(self.W.T,x.T))

	def sgd(self, x, y):
		self.W += (self.alpha * y * x).T

	def train(self, trainDataSet):
		X_parameters, Y_parameters = trainDataSet[:,:-1],trainDataSet[:,-1]
		# X_parameters = norm(X_parameters)
		X_mat = np.mat(X_parameters) # size: n * m (m = 6 now, has X_0)
		y_mat = np.mat(Y_parameters).T # size: 1 * n
		n, m = X_mat.shape
		while True:
			M = len(X_mat) # wrong classification number
			for i in range(len(X_mat)):
				if self.loss(X_mat[i], y_mat[i])  <= 0:
					self.sgd(X_mat[i], y_mat[i])
				else:
					M -= 1
			if M == 0:
				print('self.W is \n {0}'.format(self.W))
				break
		return self.W
	
	def classify(self, testDataSet):
		X_parameters, Y_parameters = testDataSet[:,:-1],testDataSet[:,-1]
		# X_parameters = norm(X_parameters)
		X_mat = np.mat(X_parameters) # size: n * m (m = 6 now, has X_0)
		y_mat = np.mat(Y_parameters).T # size: n * 1
		n, m = X_mat.shape
		M = len(X_mat) # wrong classification number
		for i in range(len(X_mat)):
			x = X_mat[i]
			if np.dot(self.W.T,x.T) <= 0 and y_mat[i] == -1:
				M -= 1
			elif np.dot(self.W.T,x.T) > 0 and y_mat[i] == 1 :
				M -= 1
		error = float(M) / len(X_mat)
		print('error rate is {0:.4f}'.format(error))
		return error


class Perceptron_dual:
	def __init__(self, alpha, b, ita, eps = 1e-8):
		self.alpha = alpha
		self.b = b
		self.ita = ita
		self.eps = eps

	
	def gram(self,X):
         return np.dot(X,X.T)
	
	def train(self, trainDataSet):
		X_parameters, Y_parameters = trainDataSet[:,1:-1],trainDataSet[:,-1]
		# X_parameters = norm(X_parameters)
		X_mat = np.mat(X_parameters) # size: n * m (m = 2 now,not has X_0)
		y_mat = np.mat(Y_parameters).T # size: n * 1
		# Y_parameters 1 * n
		n, m = X_mat.shape
		G = self.gram(X_mat)
		while True:
			M = len(X_mat) # wrong classification number
			for j in range(len(X_mat)):
				if y_mat[j] * (np.sum(self.alpha * Y_parameters * G[j].T) + self.b) <= 0:
					self.alpha[j] += self.ita
					self.b += self.ita * y_mat[j]
				else:
					M -= 1
			# print M
			if M == 0:
				print('self.alpha is \n {0}\nself.b is \n {1}'.format(self.alpha,self.b))
				break
		return self.alpha, self.b
	


def main():
	trainDataSet, testDataSet = loadData()
	# configure steplength and iterations
	alpha = 1
	perceptronTrain = Perceptron(np.zeros((trainDataSet.shape[1] - 1,1)),alpha)
	W = perceptronTrain.train(trainDataSet)
	perceptronTrain.classify(testDataSet)

	perceptronDualTrain = Perceptron_dual(np.zeros(trainDataSet.shape[1] - 1), 0, alpha)
	W = perceptronDualTrain.train(trainDataSet)
		

if __name__ == '__main__':
	main()


```



# 参考文献
[1] 统计学习方法 李航
[2] 感知机学习算法 python实现：http://www.cnblogs.com/hanahimi/p/4693289.html

----

因为我们是朋友，所以你可以使用我的文字，但请注明出处：http://alwa.info