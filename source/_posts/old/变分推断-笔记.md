---
title: 变分推断 笔记
date: 2017-08-17 13:50:36
tags:
    - DeepLearning
    - 机器学习
    - mathematic
---

# 1. 介绍

用简单的分布$q$去近似复杂分布$p$。这个简单的分布一般选用平均场(mean field)，因为这个砍断了所有变量的依赖关系，这样多个变量的分布$p$积分就可以用简单的一元积分代替。
原因是后验概率分布难求，否则直接使用EM。

<!-- more -->

# 2. 推导过程

有一个EM的式子：
$$
\log p(X) = \mathcal{L}(q) + KL(q \Vert p)
$$
其中：
$$
\mathcal{L}(q) = \int q(Z) \log \frac{p(X,Z)}{q(Z)} dZ
$$
$$
KL(q \Vert p) = - \int q(Z) \log \frac{p(Z \vert X)}{q(Z)} dZ
$$
然后我们最大化$\mathcal{L}$，这样保证$KL$散度最小。$KL(q \Vert p) \ge 0$。
$$
\max\_{q\_j} L(q) = \min\_{q\_j} KL(q \Vert \tilde p)
$$

这里的$\mathcal{L}$就是ELBO(evidence lower boud)，下面从$p(X)$从另一个方面推导一下。也就是ELBO。

$$ 
\begin{aligned}
\log p(X) &= \log \int p(X,Z) dZ \\\\
&= \log \int p(X,Z) q(Z) / q(Z) dZ\\\\
&= \log E\_q [\frac{p(X,Z)}{q(Z)}] \\\\
&\ge E\_q [\log \frac{p(X,Z)}{q(Z)}] \\\\
&= \int q(Z) \log \frac{p(X, Z)}{q(Z)} dZ \\\\
&= \mathcal{L}(q)
\end{aligned} 
$$

然后对于$KL$散度推导一下第一个EM的式子：
$$
\begin{aligned}
KL(q \Vert p) &= E\_q[ \log \frac{q(Z)}{p(Z \vert X)}]\\\\
&= E\_q[ \log \frac{q(Z) p(X)}{p(Z,X)}] \\\\
&= E\_q[ \log \frac{q(Z)}{p(Z,X)}] + \log p(X) \\\\
&= - \mathcal{L}(q) + \log p(X)
\end{aligned}
$$

求变分的过程就变成了：
1. 首先写出联合分布
2. 写出mean field形式（变分参数以及生成隐变量分布）
3. 写出ELBO
4. 偏导然后梯度学习



# 2. VBEM算法

将所有的隐藏变量和参数都放到$Z$中，但是实际情况中，隐藏变量和参数往往是不同的，所以我们其实可以采用与EM很相似的过程，将隐藏变量与参数也分开优化，这就是所谓的变分EM算法。

这里$Z$需要使用mean field认为$Z\_i$是独立的。也就是把$Z$分成$M$组：
$$
q(Z)＝\prod\_{i=1}^{M} q\_i(Z\_i)
$$

同时由于$KL$非convex要转换成求ELBO。

相当于把隐藏变量和参数优化分开。然后使用类似EM的方法。

$$
\begin{aligned}
\mathcal{L}(q) &= \int q(Z) \log \frac{p(X,Z)}{q(Z)} dZ \\\\
&= \int \prod\_{j} q\_j [\log p(X,Z) - \log \prod\_{i}^{M} q\_i] dZ \\\\
\end{aligned}
$$
只考虑一个$q\_j(Z\_j)$的情况。
$$
\begin{aligned}
\mathcal{L}(q) &= \int q\_j \left\\{\int \log p(X,Z) \prod\_{i \not = j}q\_i dZ \right\\} dZ\_j - \int q\_j \log q\_j dZ\_j + const \\\\
&= \int q\_j \log \tilde p (X,Z\_j) dZ\_j - \int q\_j \log q\_j dZ\_j + const \\\\
&= \int q\_j \frac{\tilde p (X,Z\_j)}{q\_j} dZ\_j + const \\\\
&= - KL(q\_j \Vert \tilde p(X,Z\_j)) + const
\end{aligned}
$$
其中：

$$
\begin{aligned}
\log \tilde p (X,Z\_j) = \int \log p(X,Z) \prod\_{i\not =j} q\_i dZ\_j = E\_{i \not = j}[\log p(X,Z)]
\end{aligned}
$$



# 变分自动编码机
http://www.dengfanxin.cn/?p=334
https://zhuanlan.zhihu.com/p/21741426

http://ijdykeman.github.io/ml/2016/12/21/cvae.html
http://blog.csdn.net/jackytintin/article/details/53641885
http://www.cnblogs.com/huangshiyu13/p/6209016.html
https://keras-cn.readthedocs.io/en/latest/blog/autoencoder/

# 参考
[0] 比较好的VI介绍：http://www.cs.columbia.edu/~blei/talks/2016_NIPS_VI_tutorial.pdf
[1] 变分推断与LDA：http://ariwaranosai.xyz/2014/09/13/VB-LDA/
[2] 变分推断学习笔记(1)——概念介绍：http://crescentmoon.info/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/
[3] Variational Bayes：http://www.blog.huajh7.com/variational-bayes/
[4] A Tutorial on Variational Bayesian Inference：http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf
[5] Stochastic Variational Inference
：http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf
[6] An Introduction to Variational Methods
for Graphical Models：https://people.eecs.berkeley.edu/~jordan/papers/variational-intro.pdf
[7] 如何简单易懂地理解变分推断(variational inference)？：https://www.zhihu.com/question/41765860/answer/101915528
[8] http://crescentmoon.info/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/
[9] http://ariwaranosai.xyz/2014/09/13/VB-LDA/



----

因为我们是朋友，所以你可以使用我的文字，但请注明出处：http://alwa.info
