---
title: Convolutional Neural Networks 卷积神经网络
date: 2016-03-24 21:34:47
tags:
    - 机器学习
    - mathematic
---

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="http://music.163.com/outchain/player?type=2&id=16334771&auto=0&height=66"></iframe>

# 1. CNN 介绍
卷积神经网络是人工神经网络的一种，已成为当前语音分析和图像识别领域的研究热点。它的权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复杂度，减少了权值的数量。该优点在网络的输入是多维图像时表现的更为明显，使图像可以直接作为网络的输入，避免了传统识别算法中复杂的特征提取和数据重建过程。卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。

CNNs是第一个真正成功训练多层网络结构的学习算法。它利用空间关系减少需要学习的参数数目以提高一般前向BP算法的训练性能。在CNN中，图像的一小部分（局部感受区域）作为层级结构的最低层的输入，信息再依次传输到不同的层，每层通过一个数字滤波器去获得观测数据的最显著的特征。这个方法能够获取对平移、缩放和旋转不变的观测数据的显著特征，因为图像的局部感受区域允许神经元或者处理单元可以访问到最基础的特征，例如定向边缘或者角点。

<!-- more -->

# 2. 卷积神经网络结构
![卷积神经网络](http://7xrh75.com1.z0.glb.clouddn.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_1365562094_2229.jpg)

如上图所示，卷积神经网络的最一开始输入是一个图像，然后经过三个滤波器和可加偏置进行卷积得到C1层。卷积之后的C1层可以产生三个特征映射图，然后特征映射图中每组的四个像素进行求和，加权值，加偏置，最后通过一个Sigmod(或者tan)函数得到S2层的三个特征映射图。这些映射图进行滤波得到C3层。这层和S2一起产生S4。最终的像素值被光栅化，并连接成一个长的输入向量输入到传统神经网络。

一般地，C层为特征提取层，每个神经元的输入与前一层的局部感受野相连，并提取该局部的特征，一旦该局部特征被提取后，它与其他特征间的位置关系也随之确定下来；S层是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。

此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。卷积神经网络中的每一个特征提取层（C-层）都紧跟着一个用来求局部平均与二次提取的计算层（S-层），这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。

### 2.1 感受野和权值共享
![感受野](http://7xrh75.com1.z0.glb.clouddn.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_1365562155_9356.jpg)
如上图，可以很好地说明CNN是如何减少需要训练参数的个数。比如100x100的图像，局部感受野是10x10.那么隐含层每个感受野只需要和这10x10大小的局部图像进行连接，相当于每个神经元存在$10 \times 10=100$个连接权值参数。权值共享的意思就是对于每个神经元，我都使用同一个卷积核来卷积图像。这样会导致只提取一个特征，解决方法就是多使用滤波器，比如使用100种滤波器，对应图像不用特征。

隐藏层参数个数和隐藏层的神经元是无关的，只和滤波器大小和种类多少有关系。隐藏层的神经元是由输入大小（原图像），滤波器大小和滤波器在图像的滑动步长有关。假设图像1000x1000像素，而滤波器大小是10x10，假设滤波器没有重叠，也就是步长为10，这样隐层的神经元个数就是$(1000 \times 1000) / (10 \times 10)=100 \times 100$个神经元了。这个对应一个Feature Map的神经元个数，如果有n个滤波器，那么就是n个Feature Map。

# 3. 训练
神经网络用于模式识别的主流是有指导学习网络，无指导学习网络更多的是用于聚类分析。对于有指导的模式识别，由于任一样本的类别是已知的，样本在空间的分布不再是依据其自然分布倾向来划分，而是要根据同类样本在空间的分布及不同类样本之间的分离程度找一种适当的空间划分方法，或者找到一个分类边界，使得不同类样本分别位于不同的区域内。这就需要一个长时间且复杂的学习过程，不断调整用以划分样本空间的分类边界的位置，使尽可能少的样本被划分到非同类区域中。

卷积网络在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。卷积网络执行的是有导师训练，所以其样本集是由形如：（输入向量，理想输出向量）的向量对构成的。所有这些向量对，都应该是来源于网络即将模拟的系统的实际“运行”结果。它们可以是从实际运行系统中采集来的。在开始训练前，所有的权都应该用一些不同的小随机数进行初始化。“小随机数”用来保证网络不会因权值过大而进入饱和状态，从而导致训练失败；“不同”用来保证网络可以正常地学习。实际上，如果用相同的数去初始化权矩阵，则网络无能力学习。

### 3.1 第一阶段：前向传播

+ 从样本集中取一个样本$(X,Y\_p)$，将X输入网络；
+ 计算相应的实际输出$O\_p$。

在此阶段，信息从输入层经过逐级的变换，传送到输出层。这个过程也是网络在完成训练后正常运行时执行的过程。在此过程中，网络执行的是计算（实际上就是输入与每层的权值矩阵相点乘，得到最后的输出结果）：

### 3.2 第二阶段：向后传播
+ 算实际输出$O\_p$与相应的理想输出$Y\_p$的差。
+ 按极小化误差的方法反向传播调整权矩阵。


# 参考文献
CNN讲解： http://blog.csdn.net/zouxy09/article/details/8781543
CNN代码和DEMO：http://blog.csdn.net/u012162613/article/details/43225445
卷积神经网络(CNN)在句子建模上的应用： http://www.jeyzhang.com/cnn-apply-on-modelling-sentence.html

----
　

因为我们是朋友，所以你可以使用我的文字，但请注明出处：http://alwa.info
