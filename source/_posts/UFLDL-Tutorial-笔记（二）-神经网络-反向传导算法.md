---
title: UFLDL Tutorial 笔记（二） 神经网络 反向传导算法
date: 2016-03-21 11:32:11
tags:
    - 机器学习
    - mathematic
---


# 1. 神经网络模型
　　所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。
　　
　　我们使用圆圈来表示神经网络的输入，标上“+1”的圆圈被称为偏置节点，也就是截距项。神经网络最左边的一层叫做输入层，最右的一层叫做输出层（下图中，输出层只有一个节点）。中间所有节点组成的一层叫做隐藏层，因为我们不能在训练样本集中观测到它们的值。同时可以看到，神经网络的例子中有3个输入单元（偏置单元不计在内），3个隐藏单元及一个输出单元。

　　　　　　　　![神经网络](http://7xrh75.com1.z0.glb.clouddn.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_400px-Network331.png)

　　这里使用的激活函数$f(\cdot)$是sigmoid函数。

$$f(z) = \frac{1}{1+\exp(-z)}.$$

　　使用$n\_l$来表示网络的层数，上图的$n\_l=3$，我们将第l层作为$L\_l$，于是$L\_l$是输入层，输出层是$L\_{nl}$。上图的神经网络的参数是$(W,b) = (W^{(1)},b^{(1)},W^{(2)},b^{(2)})$，其中$W\_{ij}^{(l)}$是第l层第j单元与第l+1层第i单元之间的联接参数（又叫权重）,$b\_i^{(l)}$是第l+1层第i单元的偏置项。注意，偏置单元是没有输入的，因为它们总是输出+1。同时使用$S\_l$表示第l层的节点数（无偏置单元）。
　　我们用$ a^{(l)}\_i $表示第 l 层第 i 单元的激活值（输出值）。当 l=1 时， $ a^{(1)}\_i = x\_i $，也就是第 i 个输入值（输入值的第 i 个特征）。对于给定参数集合 W,b ，我们的神经网络就可以按照函数 $h\_{W,b}(x)$ 来计算输出结果。计算过程：

$$
\begin{align}
a\_1^{(2)} &= f(W\_{11}^{(1)}x\_1 + W\_{12}^{(1)} x\_2 + W\_{13}^{(1)} x\_3 + b\_1^{(1)})  \\\\
a\_2^{(2)} &= f(W\_{21}^{(1)}x\_1 + W\_{22}^{(1)} x\_2 + W\_{23}^{(1)} x\_3 + b\_2^{(1)})  \\\\
a\_3^{(2)} &= f(W\_{31}^{(1)}x\_1 + W\_{32}^{(1)} x\_2 + W\_{33}^{(1)} x\_3 + b\_3^{(1)})  \\\\
h\_{W,b}(x) &= a\_1^{(3)} =  f(W\_{11}^{(2)}a\_1^{(2)} + W\_{12}^{(2)} a\_2^{(2)} + W\_{13}^{(2)} a\_3^{(2)} + b\_1^{(2)})
\end{align}
$$

　　我们用 $z^{(l)}\_i $表示第 l 层第 i 单元输入加权和（包括偏置单元），那么可以表示为 $z\_i^{(2)} = \sum\_{j=1}^n W^{(1)}\_{ij} x\_j + b^{(1)}\_i $，则$ a^{(l)}\_i = f(z^{(l)}\_i) $。
　　这样我们就可以得到一种更简洁的表示法。这里我们将激活函数$ f(\cdot) $ 扩展为用向量（分量的形式）来表示，即 $ f([z\_1, z\_2, z\_3]) = [f(z\_1), f(z\_2), f(z\_3)] $ ，那么，上面的等式可以更简洁地表示为：
$$
\begin{align}
z^{(2)} &= W^{(1)} x + b^{(1)} \\\\
a^{(2)} &= f(z^{(2)}) \\\\
z^{(3)} &= W^{(2)} a^{(2)} + b^{(2)} \\\\
h\_{W,b}(x) &= a^{(3)} = f(z^{(3)})
\end{align}
$$
　　这个过程称作前向传播。使用$a^{(1)} = x $作为输入层的激活的值，那么给定第l层的激活值$a^{(l)}$之后，第l+1层的激活值$a^{(l+1)}$就可以按照步骤进行计算：
　　
$$
 \begin{align}
z^{(l+1)} &= W^{(l)} a^{(l)} + b^{(l)}   \\\\
a^{(l+1)} &= f(z^{(l+1)})
\end{align}
$$
　　当然神经网络也可以有多个输出单元，以及多个隐藏层。

<!-- more -->
　
# 2. 反向传导算法
### 2.1 代价函数
　　假设我们有一个固定样本集，$ \\{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \\} $，已经包含M个样例，我们可以使用批量梯度下降法来解神经网络。对于单个样例$(x,y)$，代价函数为：
$$　　
\begin{align}
J(W,b; x,y) = \frac{1}{2} \left\| h\_{W,b}(x) - y \right\|^2.
\end{align}
$$
　　这是一个方差代价函数，一个包含M个样例的数据集，整体的代价函数就是：
$$
\begin{align}
J(W,b) &= \left[ \frac{1}{m} \sum\_{i=1}^m J(W,b;x^{(i)},y^{(i)}) \right]                        + \frac{\lambda}{2} \sum\_{l=1}^{n\_l-1} \; \sum\_{i=1}^{s\_l} \; \sum\_{j=1}^{s\_{l+1}} \left( W^{(l)}\_{ji} \right)^2 \\\\
&= \left[ \frac{1}{m} \sum\_{i=1}^m \left( \frac{1}{2} \left\| h\_{W,b}(x^{(i)}) - y^{(i)} \right\|^2 \right) \right]          + \frac{\lambda}{2} \sum\_{l=1}^{n\_l-1} \; \sum\_{i=1}^{s\_l} \; \sum\_{j=1}^{s\_{l+1}} \left( W^{(l)}\_{ji} \right)^2
\end{align}
$$　
　　以上公式中的第一项 $ J(W,b) $ 是一个均方差项。第二项是一个规则化项（也叫权重衰减项），其目的是减小权重的幅度，防止过度拟合。
　　**注意：** 一般权重衰减计算不适用偏置项$b\_i^{(l)}$。权重衰减参数 $ \lambda $用于控制公式中两项的相对重要性。在此重申一下这两个复杂函数的含义：$ J(W,b;x,y) $ 是针对单个样例计算得到的方差代价函数；$ J(W,b) $ 是整体样本代价函数，它包含权重衰减项。
　　
　　以上代价函数常常用于分类和回归问题。在分类问题中，使用y = 0或者1，来代表每种类型标签。因为 $ J(W, b) $是一个非凸函数，梯度下降法很可能会收敛到局部最优解；但是在实际应用中，梯度下降法通常能得到令人满意的结果。
　　我们目标是针对W和b求函数$J(W,b)$的最小值。所以要先初始化所有的参数为一个随机值。梯度下降法中每一次迭代都按照如下公式对参数 W 和 b 进行更新：
$$
\begin{align}
W\_{ij}^{(l)} &= W\_{ij}^{(l)} - \alpha \frac{\partial}{\partial W\_{ij}^{(l)}} J(W,b) \\\\
b\_{i}^{(l)} &= b\_{i}^{(l)} - \alpha \frac{\partial}{\partial b\_{i}^{(l)}} J(W,b)
\end{align}
$$
　　其中$ \alpha $ 是学习速率。其中关键步骤是计算偏导数。

### 2.2 反向传播算法
　　现在我们来讲使用反向传播算法计算偏导数。
　　反向传播算法来计算 $ \frac{\partial}{\partial W\_{ij}^{(l)}} J(W,b; x, y)$和 $ \frac{\partial}{\partial b\_{i}^{(l)}} J(W,b; x, y) $，这两项是单个样例$ (x,y) $的代价函数 $ J(W,b;x,y) $的偏导数。一旦我们求出该偏导数，就可以推导出整体代价函数 $ J(W,b) $的偏导数：
$$　　
\begin{align}
\frac{\partial}{\partial W\_{ij}^{(l)}} J(W,b) &=
\left[ \frac{1}{m} \sum\_{i=1}^m \frac{\partial}{\partial W\_{ij}^{(l)}} J(W,b; x^{(i)}, y^{(i)}) \right] + \lambda W\_{ij}^{(l)} \\\\
\frac{\partial}{\partial b\_{i}^{(l)}} J(W,b) &=
\frac{1}{m}\sum\_{i=1}^m \frac{\partial}{\partial b\_{i}^{(l)}} J(W,b; x^{(i)}, y^{(i)})
\end{align}
$$
　　给定一个样例$ (x,y)$，我们首先进行“前向传导”运算，计算出网络中所有的激活值，包括 $ h\_{W,b}(x) $的输出值。之后，针对第 l 层的每一个节点 i，我们计算出其“残差” $ \delta^{(l)}\_i$，该残差表明了该节点对最终输出值的残差产生了多少影响。对于最终的输出节点，我们可以直接算出网络产生的激活值与实际值之间的差距，我们将这个差距定义为 $ \delta^{(n\_l)}\_i $（第 $n\_l$ 层表示输出层）。对于隐藏单元我们如何处理呢？我们将基于节点（第$ l+1$ 层节点）残差的加权平均值计算 $ \delta^{(l)}\_i$，这些节点以$ a^{(l)}\_i$ 作为输入。下面将给出反向传导算法结论，具体推导可以看[这里](http://ufldl.stanford.edu/wiki/index.php/反向传导算法):
$$
\begin{align}
\frac{\partial}{\partial W\_{ij}^{(l)}} J(W,b; x, y) &= a^{(l)}\_j \delta\_i^{(l+1)} \\\\
\frac{\partial}{\partial b\_{i}^{(l)}} J(W,b; x, y) &= \delta\_i^{(l+1)}.
\end{align}
$$




# 参考文献
http://ufldl.stanford.edu/wiki/index.php/UFLDL\_Tutorial


----
　

因为我们是朋友，所以你可以使用我的文字，但请注明出处：http://alwa.info
