---
title: 多层感知机推导以及代码实现
date: 2016-04-19 16:18:55
tags:
    - 机器学习
    - mathematic
---




# 1. 定义
　　前馈神经网络也经常称为多层感知机（MLP），但是多层感知机的叫法并不是特别合理。因为前馈神经网络其实是由多层的logistics回归模型组成，而不是由多层的感知机（不连续的非线性函数）组成。

　　多层感知器（Multilayer Perceptron,缩写MLP）是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。一种被称为反向传播算法的监督学习方法常被用来训练MLP。MLP是感知器的推广，克服了感知器不能对线性不可分数据进行识别的弱点。

　　多层感知器网络的一个实现是使用具有非线性连续变换函数的误差反向传播(Error Back Propagation)算法。Error Back Propagation算法的简称就是BP算法，以BP算法实现的多层感知器网络就是BP网络。
所以，BP网络本质上并不是一个新的网络，而是使用BP学习算法的多层感知器网络。

<!-- more -->


# 2. 推导

　　人工神经元使用一个非线性激活函数，输出一个活性值。假定神经元接受n个输入$X = (x\_1,X\_2,\dots,X\_n$，用状态z表示一个神经元所获得输入信号x的加权和，输出为该神经元的活性值a，定义如下：
$$
\begin{align}
z &= W^T X + b \\\\
a &= f(z)
\end{align}
$$

　　其中$W$是n维权重向量，b是偏置项。典型的激活函数有sigmoid型函数、非线性斜面函数等。如果我们设定激活函数f为0或者1的阶跃函数，人工神经元就是感知机。

　　为了增强网络的表达能力，一般需要引入连续的非线性激活函数。连续的非线性激活函数是可导的，所以可以用最优化方法求解。传统神经网络最常用的激活函数分别是sigmoid型函数。


# 2. 前馈神经网络
　　前馈神经网络是最早发明的简单人工神经网络。第一层叫做输入层，最后一层叫输出层，其他中间层叫做隐藏层。整个网络中无反馈。信号层向输出层单项传播，可用一个有向无环图表示。
　　
　　我们使用如下记号描述网络。

　　$L$: 神经网络层数
　　$n^l$：表示第$l$层神经元的个数
　　$f\_l(\cdot)$：表示第$l$层神经元的激活函数
　　$W^{(l)} \in R^{n^l \times n ^{l-1}}$：表示第$l-1$层到第$l$层的权重矩阵
　　$b^{(l)} \in R^{n^l}$：表示第$l-1$层到第$l$层的偏置
　　$z^{(l)} \in R^{n^l}$：表示第$l$层的神经元状态
　　$a^{(l)} \in R^{n^l}$：表示第$l$层的神经元活性值


　　前馈神经网络通过下面公式进行信息传播。
$$
\begin{align}
z ^{(l)} &= W^{(l)} \cdot a^{(l-1)} + b^{(l)} \\\\
a ^{(l)} &= f\_l(z^{(l)}) 
\end{align}
$$
　　公式也可以写成
$$
z ^{(l)} = W^{(l)} \cdot f\_l(z^{(l-1)}) + b^{(l)} 
$$

　　前馈神经网络通过逐层信息传递，最终得到输出$a^L$。
$$
X = a^{(0)} \to z^{(1)} \to a^{(1)} \to z^{(2)} \to \dots \to a^{(L-1)} \to z^{(L)} \to a^{(L)} = y
$$


# 3. 反向传播
　　给定一组样本$(X{(i)},y{(i)}), 1 \le i \le N $，前馈神经网络的输出为$f(X|W,b)$目标函数为：
$$
\begin{align}
J(W,b) &= \sum\_{i=1}^{N} L(y^{(i)},f(X^{(i)}| W,b)) + \frac{1}{2} \lambda ||W||\_{F}^2 \\\\
&= \sum\_{i=1}^{N} J(W,b;X{(i)},y^{(i)}) + \frac{1}{2} \lambda ||W||\_{F}^2
\end{align}
$$
　　这里$W$和$b$包含了每一层的权重举证和偏置向量。其中$||W||\_{F}^2 = \sum\_{l=1}^{L} \sum\_{j=1}^{n^{l+1}} \sum\_{1=1}^{n^l} W\_{ij}^{(l)}$

　　如果使用梯度下降方法来最小化$J(W,b;X{(i)},y^{(i)})$，我们可以使用如下方法更新参数：
$$
\begin{align}
W^{(l)} &= W^{(l)} - \alpha \frac{\partial J(W,b) }{\partial W^{(l)}} ,\\\\
&=  W^{(l)} - \alpha \sum\_{i=1}^{N} ( \frac{J(W,b;X^{(i)},y^{(i)})}{\partial W^{(l)}}  )- \lambda W \\\\
b^{(l)} &= b^{(l)} - \alpha \frac{\partial J(W,b) }{\partial b^{(l)}} ,\\\\
&=  b^{(l)} - \alpha \sum\_{i=1}^{N} ( \frac{J(W,b;X^{(i)},y^{(i)})}{\partial b^{(l)}}  )\\\\
\end{align}
$$

　　$\lambda$是参数更新率。
$$
\frac{\partial J(W,b;X,y)}{\partial W\_{ij}^{(l)}} = tr((\frac{J(W,b;X,y)}{\partial z^{(l)}})^T \frac{\partial z^{(l)}}{\partial W\_{ij}^{(l)}})
$$
　　对于第l层，我们定义一个误差项$\delta ^{(l)} = \frac{J(W,b;X,y)}{\partial z^{(l)}} \in R^{n^{(l)}}$是目标函数关于第l层的神经元$z^{(l)}$的偏导数。$\delta ^{(l)}$也反映了最终的输出对第l层神经元对最终误差的敏感程度。

$$
\begin{align}
z ^{(l)} &= W^{(l)} a^{(l-1)} + b^{(l)} \\\\
\frac{\partial z^{(l)}}{\partial W\_{ij}^{(l)}} &= \frac{\partial (W^{(l)} a^{(l-1)} + b^{(l)}) }{\partial W\_{ij}^{(l)}} \\\\ 
\frac{ \partial J(W,b;X,y)}{\partial W\_{ij}^{(l)}} &= \delta \_{i}^{(l)} a\_j^{(l-1)}
\end{align}
$$
　　所以可得
$$
\begin{align}
\frac{ \partial J(W,b;X,y)}{\partial W^{(l)}} &= \delta \_{i}^{(l)} (a\_j^{(l-1)})^T \\\\
\frac{ \partial J(W,b;X,y)}{\partial b^{(l)}} &= \delta \_{i}^{(l)}  
\end{align}
$$

　　现在来看第l层的误差项$\delta^{(l)}$
$$
\begin{align}
\delta^{(l)} &\triangleq \frac{J(W,b;X,y)}{\partial z^{(l)}} \\\\
&= \frac{\partial a^{(l)}}{\partial z^{(l)}} \cdot \frac{\partial z^{(l+1)}}{\partial a^{(l)}} \cdot \frac{J(W,b;X,y)}{\partial z^{(l+1)}} \\\\
&= diag(f\_l'(z^{(l)})) \cdot (W^{(l+1)})^T \cdot \delta ^{(l+1)} \\\\
&= f\_l'(z^{(l)}) \odot ((W^{(l+1)})^T \delta ^{(l+1)})
\end{align}
$$
$\odot$是向量点积运算符，表示每个元素相乘。
　　因此，前馈神经网络的训练过程可以分为以下三步：（1）先前馈计算每一层的状态和激活值，直到最后一层，（2）反向传播计算每一层的误差，（3）计算每一层参数的偏导数，并更新参数。


# 4. 梯度消失原因
　　因为每次误差从输出层反向传播时，每一层都要乘该层的激活函数导数。因为值域小于1，误差经过每一层传递都会不断衰减。


# 参考文献
[1] 链式法则：https://zh.wikipedia.org/zh/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99
[2] 漫谈ANN(2)：BP神经网络：http://hahack.com/reading/ann2/#柳暗花明又一村ann研究的复苏和bp神经网络的诞生

----

因为我们是朋友，所以你可以使用我的文字，但请注明出处：http://alwa.info