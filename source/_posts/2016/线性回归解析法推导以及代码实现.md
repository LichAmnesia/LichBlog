---
title: 线性回归解析法推导以及代码实现
date: 2016-04-09 18:07:35
tags:
    - 机器学习
    - mathematic
---

本文介绍线性回归定义以及推导过程，最后使用python实现了这个算法。

<!-- more -->

# 1. 线性回归定义
　　如果输入$X$是列向量，目标$y$也是连续值，预测函数输出也是连续值。那就是一个回归问题。
$$
f(X) = W^TX + b
$$
　　这就是一个线性回归问题。简单起见可以把公式改为：
$$
f(X) = \hat W ^ T \hat X
$$
　　其中$\hat W$和$\hat X$称之为**增广权重向量**和**增广特征向量**


# 2. 解析方法解
　　线性回归的损失函数通常定义为：
$$
L(y,f(X,W)) = (y - f(X,W))^2
$$
　　模型的经验风险为：
$$
\begin{align}
R(Y,f(X,W)) &= \sum\_i^{N}L(y^{(i)},f(x^{(i)},W))   \\\\ 
 &= \sum\_i^{N} (W^Tx^{(i)} - y)^2  \\\\
 &= (X^TW - Y)^2  \\\\
 &= (X^TW - Y)^T (X^TW - Y)
\end{align}
$$
　　如果最小化$R(Y,f(X,W))$，要计算$R(Y,f(X,W))$对$W$的导数:（可以把这个平方了之后再进行求导，然后使用平方转化为$A^T$矩阵）
$$
\begin{align}
\frac{\partial R(Y, f(X,W))}{\partial W} &= \frac{\partial (X^TW - Y)^2}{\partial W} \\\\ 
&=  X (X^TW - Y)
\end{align}
$$
　　让这个$\frac{\partial R(Y, f(X,W))}{\partial W}$值为0，那么就能得到解析解：
$$
W = (XX^T)^{-1}XY
$$

# 3. 代码实现
　　整个代码放到我的[GitHub](https://github.com/LichAmnesia/MachineLearningTools)上了。这个例子是house price和Year相关性的例子。
### 3.1 输入数据
>2000 2.000
2001 2.500
2002 2.900
2003 3.147
2004 4.515
2005 4.903
2006 5.365
2007 5.704
2008 6.853
2009 7.971
2010 8.561
2011 10.000
2012 11.280
2013 12.900

### 3.2 代码实现
　　依赖numpy和matplotlib。如果没有安装，可以参考我之前写的[Theano安装配置](http://alwa.info/2016/03/28/Theano-%E9%85%8D%E7%BD%AE-%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/)的第一部分。

```python
# -*- coding: utf-8 -*-
# @Author: Lich_Amnesia  
# @Email: alwaysxiaop@gmail.com
# @Date:   2016-04-08 18:35:50
# @Last Modified time: 2016-04-08 19:38:38
# @FileName: LinearRegression.py

import numpy as np
import matplotlib.pyplot as plt
import sys,os

# use numpy load from txt function to load data
def loadData():
	file_path = 'data/LR_in.txt'
	file = open(file_path)
	data_set = np.loadtxt(file)
	X0 = np.array([1.0 for i in range(data_set.shape[0])])
	return np.c_[X0,data_set[:,:-1]],data_set[:,-1] 

# use X^T * X to calculate the answer
def calculateMethod(X_parameters, Y_parameters):
	X = np.mat(X_parameters)
	# import! this y should be Y.T, you can print it to find the reason
	y = np.mat(Y_parameters).T
	tmp1 = np.dot(X.T,X).I
	tmp2 = np.dot(X.T,y)
	theta = np.dot(tmp1,tmp2)
	theta = np.array(theta)
	print(theta)
	return theta

# use calculated theta, it will returrn predict Y
def predictOut(X_parameters, theta):
	X = np.mat(X_parameters)
	theta = np.mat(theta)
	out = np.dot(X,theta)
	return out

# use matplotlib to draw X-Y axis points
def draw(X_parameters, Y_parameters,theta):
	plt.scatter(X_parameters[:,-1],Y_parameters,color='blue')
	Y_predict_out = predictOut(X_parameters,theta)
	plt.plot(X_parameters[:,-1],Y_predict_out,color='r',linewidth=4)
	plt.xlabel('Year')
	plt.ylabel('House Price')
	plt.show()
	return

def main():
	X_parameters, Y_parameters = loadData()
	theta = calculateMethod(X_parameters,Y_parameters)
	draw(X_parameters,Y_parameters,theta)
	# print(X_parameters)
	return

if __name__ == '__main__':
	main()
```



# 参考文献
[1] 神经网络与深度学习讲义20151211


----

因为我们是朋友，所以你可以使用我的文字，但请注明出处：http://alwa.info